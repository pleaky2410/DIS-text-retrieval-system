{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus json\n",
    "import json\n",
    "\n",
    "print('Load corpus.json')\n",
    "with open('../../data/corpus.json/corpus.json', 'r') as f:\n",
    "    documents = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Only keep documents of given lang\n",
    "import gc\n",
    "\n",
    "documents_en = [doc for doc in documents if doc['lang'] == 'en']\n",
    "del documents\n",
    "documents = documents_en\n",
    "del documents_en\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: only select a random subset of documents from corpus\n",
    "import random\n",
    "import gc\n",
    "\n",
    "random.seed(42)\n",
    "num_documents_to_select = 50000\n",
    "\n",
    "selected_documents = random.sample(documents, num_documents_to_select)\n",
    "\n",
    "# Print some examples to check\n",
    "for i, doc in enumerate(selected_documents[:5]):\n",
    "    print(f\"Document {i+1}: {doc['docid']} - {doc['text'][:100]}...\")\n",
    "\n",
    "# Extract docids from the selected documents\n",
    "selected_docids = [doc['docid'] for doc in selected_documents]\n",
    "\n",
    "# Write the selected docids to a file\n",
    "docids_file_path = 'selected_docids.json'\n",
    "with open(docids_file_path, 'w') as f:\n",
    "    json.dump(selected_docids, f)\n",
    "print(f\"Selected docids written to {docids_file_path}\")\n",
    "\n",
    "# Replace documents variable and clean memory\n",
    "del documents\n",
    "documents = selected_documents\n",
    "del selected_documents\n",
    "del selected_docids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create dict to match document index and corresponding docid\n",
    "import gc\n",
    "docids = [doc['docid'] for doc in documents]\n",
    "\n",
    "doc_index_to_docid = {index: doc_id for index, doc_id in enumerate(docids)}\n",
    "# write to disk\n",
    "with open('doc_index_to_docid_en.json', 'w') as f: # EN !\n",
    "    json.dump(doc_index_to_docid, f)\n",
    "\n",
    "del doc_index_to_docid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from docs and TEXT PREPROCESSING APPLIED\n",
    "from preprocessing import clean_text\n",
    "print('Extract text from docs')\n",
    "texts = [clean_text(doc['text']) for doc in documents] # PREPROCESSING APPLIED\n",
    "\n",
    "del documents\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# # Save docs in temp file\n",
    "# import tempfile\n",
    "# print('Save docs texts in temp file')\n",
    "# with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "#     temp_file_name = temp_file.name\n",
    "#     for text in texts:\n",
    "#         temp_file.write((text + '\\n').encode('utf-8'))\n",
    "\n",
    "# Save docs in a normal file\n",
    "temp_file_name = 'preprocessed_texts_en.txt'\n",
    "print('Save docs texts in a text file')\n",
    "with open(temp_file_name, 'w', encoding='utf-8') as file:\n",
    "    for text in texts:\n",
    "        file.write(text + '\\n')\n",
    "\n",
    "\n",
    "print('Delete texts variable')\n",
    "del texts\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import gc\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "print('Start fasttext model training')\n",
    "model = fasttext.train_unsupervised(temp_file_name, thread=cpu_count())\n",
    "\n",
    "\n",
    "# Close and remove the temporary file\n",
    "# temp_file.close()\n",
    "# os.remove(temp_file_name)\n",
    "\n",
    "# Save the trained model\n",
    "model_name = \"fasttext_unsupervised_skipgram_dim100_en\"\n",
    "print(f'Save the trained model to models/{model_name}.bin')\n",
    "model.save_model(f\"models/{model_name}.bin\")\n",
    "\n",
    "\n",
    "print('Delete model variable')\n",
    "del model\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
