{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple test\n",
    "Find the most similar terms for a given term. The similarity between two terms is defined as the cosine similarity between their corresponding word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model vocabulary and embeddings \n",
    "import fasttext\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "model = fasttext.load_model(\"models/fasttext_unsupervised_cbow_dim100_mini.bin\")\n",
    "\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])\n",
    "\n",
    "# Clean memory\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(input_term, word_embeddings, vocabulary, num_terms=5):\n",
    "    # Create dict to associate embedding to each term in vocabulary\n",
    "    term_embeddings_dict = {} \n",
    "    for i,term in enumerate(vocabulary):\n",
    "        term_embeddings_dict[term] = word_embeddings[i]\n",
    "    \n",
    "    # Find input_term in embeddings dict\n",
    "    if input_term not in term_embeddings_dict:\n",
    "        return \"Term not in the vocabulary\"\n",
    "    input_term_embedding = term_embeddings_dict[input_term]\n",
    "\n",
    "    # Calculate similarity with each term in vocabulary\n",
    "    term_similarities = []\n",
    "    for term, embedding in term_embeddings_dict.items():\n",
    "        term_similarities.append([term, cosine_similarity(input_term_embedding.reshape((1,-1)), embedding.reshape((1,-1)))]) # reshape embedding into 2D array with 1 line as expected by cosine_similarity function\n",
    "        \n",
    "    sorted_terms = sorted(term_similarities, key = lambda x: -1 * x[1])[0:num_terms] # sort by decreasing similarity score, select num_terms first elements\n",
    "\n",
    "    return sorted_terms\n",
    "    \n",
    "\n",
    "find_most_similar('ireland', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean memory\n",
    "del vocabulary\n",
    "del word_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with a given query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model, corpus data and the aggregated vectors for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fasttext_unsupervised_cbow_dim300_preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model vocabulary and embeddings \n",
    "import fasttext\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "model = fasttext.load_model(f\"models/{model_name}.bin\")\n",
    "\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])\n",
    "\n",
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "\n",
    "# Clean memory\n",
    "del vocabulary\n",
    "del word_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aggregated vectors for each document from disk\n",
    "import pickle\n",
    "\n",
    "aggregated_docs_vectors_file = f'aggregated_docs_vectors/adv_{model_name}.pkl'\n",
    "\n",
    "# Load aggregated_docs_vectors from disk\n",
    "with open(aggregated_docs_vectors_file, 'rb') as f:\n",
    "    aggregated_docs_vectors = pickle.load(f)\n",
    "print(\"Loaded aggregated_docs_vectors from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dict to match document index and corresponding docid\n",
    "import json\n",
    "with open('doc_index_to_docid.json', 'r') as f:\n",
    "    doc_index_to_docid = json.load(f)\n",
    "doc_index_to_docid = {int(key): value for key, value in doc_index_to_docid.items()} # reconvert keys to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate the query\n",
    "Aggregate the query and find the most similar documents using cosine distance between the query's vector and document's aggregated vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return np.array(vlist).max(axis=0)\n",
    "    elif aggfunc == 'min':\n",
    "        return np.array(vlist).min(axis=0)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(vlist).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])\n",
    "\n",
    "def aggregate_query(query, aggfunc):\n",
    "    # Raise an error message for the case when there is no words in the query that is included in the vocabulary\n",
    "    # This should return a vector of shape (1, word_embeddings.shape[1])\n",
    "    tokens = fasttext.tokenize(query)\n",
    "\n",
    "    vlist = []\n",
    "    for token in tokens:\n",
    "        if token in vector_dict:\n",
    "            vlist.append(vector_dict[token])\n",
    "        else:\n",
    "            print(f\"{token} is not in the vocabulary\")\n",
    "            vlist.append(model.get_word_vector(token)) # use n-grams of word to obtain a vector for this out-of-vocabulary word\n",
    "\n",
    "    return aggregate_vector_list(vlist, aggfunc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    # Calculate the similarity with each document vector. \n",
    "    sim = cosine_similarity(query_vector.reshape((1,-1)), aggregated_docs_vectors[aggfunc])\n",
    "    \n",
    "    # Rank the document vectors according to their cosine similarity with the query vector and return topk indexes\n",
    "    indexes = np.argsort(sim, axis=-1, kind='quicksort', order=None) # This is sorted in ascending order, along last axis\n",
    "    indexes = indexes[0]\n",
    "    indexes = indexes[::-1] # Convert to descending\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean'):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    indexes = indexes[0:topk]\n",
    "    docids_retrieved = [doc_index_to_docid[index] for index in indexes]\n",
    "    return docids_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import clean_text\n",
    "query = \"What is the syntax for the shorthand of the conditional operator in PHP 5.3?\"\n",
    "cleaned_query = clean_text(query) # APPLY PREPROCESSING\n",
    "docids_retrieved = search_vec_embeddings(query=cleaned_query, aggfunc = 'mean')\n",
    "print(f\"Docids retrieved : {docids_retrieved}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate recall on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate recall@10\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    retrieved_set = set(retrieved_docs[:k])\n",
    "    relevant_set = set(relevant_docs)\n",
    "    intersection = retrieved_set.intersection(relevant_set)\n",
    "    recall = len(intersection) / len(relevant_set)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dev set\n",
    "dev_set_path = '../../data/dev.csv'\n",
    "dev_set = pd.read_csv(dev_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive/Negative docs to list\n",
    "def docs_to_list(docs):\n",
    "    if isinstance(docs, str):\n",
    "        if docs.startswith('[') and docs.endswith(']'):\n",
    "            return eval(docs)\n",
    "        else:\n",
    "            return [docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: select only queries whose postitive document is in the randomly selected subset\n",
    "\n",
    "docids_file_path = 'selected_docids.json'\n",
    "\n",
    "with open(docids_file_path, 'r') as f:\n",
    "    selected_docids = json.load(f)\n",
    "\n",
    "dev_set['positive_docs'] = dev_set['positive_docs'].apply(docs_to_list)\n",
    "dev_set['negative_docs'] = dev_set['negative_docs'].apply(docs_to_list)\n",
    "\n",
    "# Filter the queries to keep only those with docid in selected_docids\n",
    "filtered_dev_set = dev_set[dev_set['positive_docs'].apply(lambda docs: any(doc in selected_docids for doc in docs))]\n",
    "\n",
    "# Print some examples to check\n",
    "print(filtered_dev_set.head())\n",
    "\n",
    "# Replace variable\n",
    "del dev_set\n",
    "dev_set = filtered_dev_set\n",
    "del filtered_dev_set\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall@10 for each query in dev set\n",
    "recalls = []\n",
    "lang_recalls = {}\n",
    "for index, row in dev_set.iterrows():\n",
    "    query = row['query']\n",
    "    lang = row['lang']\n",
    "    cleaned_query = clean_text(query) # APPLY PREPROCESSING\n",
    "    positive_docs = docs_to_list(row['positive_docs']) # convert str to python list\n",
    "    retrieved_docs = search_vec_embeddings(cleaned_query, topk=10, aggfunc='mean')\n",
    "    recall = calculate_recall_at_k(retrieved_docs, positive_docs, k=10)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    # Add recall to specific langage\n",
    "    if lang not in lang_recalls:\n",
    "        lang_recalls[lang] = []\n",
    "    lang_recalls[lang].append(recall)\n",
    "\n",
    "# Calculate average recall\n",
    "mean_recall_at_10 = np.mean(recalls)\n",
    "print(f\"Mean Recall@10: {mean_recall_at_10:.4f}\")\n",
    "\n",
    "# Calculate average recall for each language\n",
    "for lang, lang_recall_list in lang_recalls.items():\n",
    "    mean_lang_recall = np.mean(lang_recall_list)\n",
    "    print(f\"Mean Recall@10 for {lang}: {mean_lang_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "try:\n",
    "    model_name\n",
    "except NameError:\n",
    "    model_name = \"fasttext_unsupervised_cbow_dim100\"\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"word_embedding\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=model_name):\n",
    "    mlflow.log_param(\"framework\", \"fasttext\")\n",
    "    mlflow.log_param(\"method\", \"unsupervised\")\n",
    "    mlflow.log_param(\"model\", \"cbow\")\n",
    "    mlflow.log_param(\"input\", \"full_corpus\")\n",
    "\n",
    "    mlflow.log_metric(\"dimension\", 300)\n",
    "    mlflow.log_metric(\"recall_at10_dev\", mean_recall_at_10)\n",
    "    for lang, lang_recall_list in lang_recalls.items():\n",
    "        mean_lang_recall = np.mean(lang_recall_list)\n",
    "        mlflow.log_metric(f\"recall_at10_dev_{lang}\", mean_lang_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "test_set_path = '../../data/test.csv'\n",
    "test_set = pd.read_csv(test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents retrieved for each query in test set\n",
    "predicted_docs = []\n",
    "for index, row in test_set.iterrows():\n",
    "    query_id = row['id']\n",
    "    query = row['query']\n",
    "    retrieved_docs = search_vec_embeddings(query, topk=10, aggfunc='mean')\n",
    "    predicted_docs.append((query_id, retrieved_docs))\n",
    "\n",
    "# Create Dataframe with results\n",
    "results_df = pd.DataFrame(predicted_docs, columns=['id', 'docids'])\n",
    "\n",
    "# Save to csv\n",
    "results_df.to_csv('predicted_docs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
