{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIS Project 1: Document Retrieval\n",
    "EPFL CS-423 (Distributed Information Systems)\n",
    "\n",
    "Team: \"TIIM\" - AndrÃ© Santo (376762), Vincent Fiszbin (394790), Rasmus Veski (395667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:08:47.128273Z",
     "iopub.status.busy": "2024-11-01T17:08:47.127787Z",
     "iopub.status.idle": "2024-11-01T17:09:10.997992Z",
     "shell.execute_reply": "2024-11-01T17:09:10.996213Z",
     "shell.execute_reply.started": "2024-11-01T17:08:47.128209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyStemmer in /opt/conda/lib/python3.10/site-packages (2.2.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from konlpy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from konlpy) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.10/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U PyStemmer\n",
    "%pip install -U konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:11.001276Z",
     "iopub.status.busy": "2024-11-01T17:09:11.000808Z",
     "iopub.status.idle": "2024-11-01T17:09:12.104926Z",
     "shell.execute_reply": "2024-11-01T17:09:12.103678Z",
     "shell.execute_reply.started": "2024-11-01T17:09:11.001209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "import os\n",
    "from typing import Dict, List, NamedTuple\n",
    "import jax.lax\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.sparse as sp\n",
    "import re\n",
    "from typing import Dict, List, Union, NamedTuple, Tuple\n",
    "import Stemmer\n",
    "from konlpy.tag import Kkma\n",
    "import pickle\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath('/kaggle/input/stopwords'))\n",
    "\n",
    "from stopwords import (\n",
    "    STOPWORDS_EN,\n",
    "    STOPWORDS_EN_PLUS,\n",
    "    STOPWORDS_GERMAN,\n",
    "    STOPWORDS_DUTCH,\n",
    "    STOPWORDS_FRENCH,\n",
    "    STOPWORDS_SPANISH,\n",
    "    STOPWORDS_PORTUGUESE,\n",
    "    STOPWORDS_ITALIAN,\n",
    "    STOPWORDS_RUSSIAN,\n",
    "    STOPWORDS_SWEDISH,\n",
    "    STOPWORDS_NORWEGIAN,\n",
    "    STOPWORDS_CHINESE,\n",
    "    STOPWORDS_ARABIC,\n",
    "    STOPWORDS_KOREAN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:12.107506Z",
     "iopub.status.busy": "2024-11-01T17:09:12.106916Z",
     "iopub.status.idle": "2024-11-01T17:09:12.149983Z",
     "shell.execute_reply": "2024-11-01T17:09:12.148730Z",
     "shell.execute_reply.started": "2024-11-01T17:09:12.107461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Specify data types for memory efficiency and performance\n",
    "FLOAT_TYPE = \"float32\"\n",
    "INT_TYPE = \"int32\"\n",
    "\n",
    "class TokenData(NamedTuple):\n",
    "    ids: List[List[int]]\n",
    "    vocab: Dict[str, int]\n",
    "\n",
    "class SearchResults(NamedTuple):\n",
    "    documents: np.ndarray\n",
    "    scores: np.ndarray\n",
    "\n",
    "def compute_doc_frequencies(tokenized_corpus, unique_tokens, show_progress=True) -> dict:\n",
    "    \"\"\"\n",
    "    Compute document frequencies (DF) for each token in the corpus.\n",
    "\n",
    "    Parameters:\n",
    "    tokenized_corpus (List[List[int]]): List of tokenized documents.\n",
    "    unique_tokens (List[int]): List of unique token IDs.\n",
    "    show_progress (bool): Whether to show a progress bar.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary with token IDs as keys and document frequencies as values.\n",
    "    \"\"\"\n",
    "    unique_tokens = set(unique_tokens)\n",
    "    doc_freqs = {token: 0 for token in unique_tokens}\n",
    "    for doc_tokens in tqdm(tokenized_corpus, disable=not show_progress, desc=\"Counting Tokens\"):\n",
    "        for token in unique_tokens.intersection(doc_tokens):\n",
    "            doc_freqs[token] += 1\n",
    "    return doc_freqs\n",
    "\n",
    "def create_idf_array(doc_freqs: dict, total_docs: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the Inverse Document Frequency (IDF) for each token using the document frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    doc_freqs (dict): Dictionary with token IDs as keys and document frequencies as values.\n",
    "    total_docs (int): Total number of documents in the corpus.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Array of IDF values.\n",
    "    \"\"\"\n",
    "    idf_array = np.zeros(len(doc_freqs), dtype=FLOAT_TYPE)\n",
    "    for token_id, df in doc_freqs.items():\n",
    "        idf_array[token_id] = math.log(1 + (total_docs - df + 0.5) / (df + 0.5)) # Lucene variant\n",
    "    return idf_array\n",
    "\n",
    "def compute_term_frequency(tf_array, doc_len, avg_doc_len, k1, b):\n",
    "    \"\"\"\n",
    "    Compute term frequency using the BM25 formula.\n",
    "\n",
    "    Parameters:\n",
    "    tf_array (np.ndarray): Array of term frequencies.\n",
    "    doc_len (int): Length of the document.\n",
    "    avg_doc_len (float): Average document length in the corpus.\n",
    "    k1 (float): BM25 parameter.\n",
    "    b (float): BM25 parameter.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Array of term frequency values.\n",
    "    \"\"\"\n",
    "    return tf_array / (k1 * ((1 - b) + b * doc_len / avg_doc_len) + tf_array) # Robertson variant\n",
    "\n",
    "def get_token_counts(token_ids):\n",
    "    \"\"\"\n",
    "    Get token counts from a list of token IDs.\n",
    "\n",
    "    Parameters:\n",
    "    token_ids (List[int]): List of token IDs.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Arrays of unique token IDs and their counts.\n",
    "    \"\"\"\n",
    "    token_counter = Counter(token_ids)\n",
    "    return np.array(list(token_counter.keys()), dtype=INT_TYPE), np.array(list(token_counter.values()), dtype=FLOAT_TYPE)\n",
    "\n",
    "def create_score_matrix(corpus_token_ids, idf_array, avg_doc_len, doc_freqs, k1, b, show_progress=True):\n",
    "    \"\"\"\n",
    "    Create the BM25 score matrix for the corpus.\n",
    "    Compute the BM25 scores for each token in each document of the corpus, the scores along with \n",
    "    the corresponding document and vocabulary indices.\n",
    "\n",
    "    Parameters:\n",
    "    corpus_token_ids (List[List[int]]): List of tokenized documents.\n",
    "    idf_array (np.ndarray): Array of IDF values.\n",
    "    avg_doc_len (float): Average document length in the corpus.\n",
    "    doc_freqs (dict): Dictionary with token IDs as keys and document frequencies as values.\n",
    "    k1 (float): BM25 parameter.\n",
    "    b (float): BM25 parameter.\n",
    "    show_progress (bool): Whether to show a progress bar.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]: Arrays of scores, document indices, and vocabulary indices.\n",
    "    \"\"\"\n",
    "    array_size = sum(doc_freqs.values())\n",
    "    \n",
    "    # Initialize arrays to store scores, document indices, and vocabulary indices\n",
    "    scores = np.empty(array_size, dtype=FLOAT_TYPE)\n",
    "    doc_indices = np.empty(array_size, dtype=INT_TYPE)\n",
    "    vocab_indices = np.empty(array_size, dtype=INT_TYPE)\n",
    "    \n",
    "    i = 0\n",
    "    # Iterate over each document in the corpus\n",
    "    for doc_idx, token_ids in enumerate(tqdm(corpus_token_ids, desc=\"Computing Scores\", disable=not show_progress)):\n",
    "        doc_len = len(token_ids)\n",
    "        \n",
    "        # Get term frequencies in the current document\n",
    "        vocab_indices_doc, tf_array = get_token_counts(token_ids)\n",
    "        \n",
    "        # Compute BM25 scores (for each token) for the current document\n",
    "        scores_doc = idf_array[vocab_indices_doc] * compute_term_frequency(tf_array, doc_len, avg_doc_len, k1, b)\n",
    "        \n",
    "        doc_len = len(scores_doc)\n",
    "        \n",
    "        # Determine the start and end indices for the current document scores in the arrays\n",
    "        start = i\n",
    "        end = i + doc_len\n",
    "        i = end  # position where next document's scores will start\n",
    "        \n",
    "        # Store the computed scores and corresponding indices in the arrays\n",
    "        doc_indices[start:end] = doc_idx\n",
    "        vocab_indices[start:end] = vocab_indices_doc\n",
    "        scores[start:end] = scores_doc\n",
    "    \n",
    "    return scores, doc_indices, vocab_indices\n",
    "\n",
    "def tokens_to_strings(token_data: TokenData) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Convert token IDs back to strings.\n",
    "\n",
    "    Parameters:\n",
    "    token_data (TokenData): TokenData object containing token IDs and vocabulary.\n",
    "\n",
    "    Returns:\n",
    "    List[List[str]]: List of documents with token strings.\n",
    "    \"\"\"\n",
    "    reverse_vocab = {v: k for k, v in token_data.vocab.items()}\n",
    "    return [[reverse_vocab[token_id] for token_id in doc_ids] for doc_ids in token_data.ids]\n",
    "\n",
    "def get_top_k(query_scores, k):\n",
    "    \"\"\"\n",
    "    Get the top k scores and their indices.\n",
    "\n",
    "    Parameters:\n",
    "    query_scores (np.ndarray): Array of query scores.\n",
    "    k (int): Number of top scores to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray]: Arrays of top k scores and their indices.\n",
    "    \"\"\"\n",
    "    topk_scores, topk_indices = jax.lax.top_k(query_scores, k)\n",
    "    return np.asarray(topk_scores), np.asarray(topk_indices)\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        \"\"\"\n",
    "        Initialize the BM25 object with parameters.\n",
    "\n",
    "        Parameters:\n",
    "        k1 (float): BM25 parameter.\n",
    "        b (float): BM25 parameter.\n",
    "        \"\"\"\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "    def compute_relevance_scores(self, query_token_ids):\n",
    "        \"\"\"\n",
    "        Compute relevance scores for a query. Using precomputed BM25 scores.\n",
    "\n",
    "        Parameters:\n",
    "        data (np.ndarray): Array of score data.\n",
    "        indptr (np.ndarray): Index pointer array.\n",
    "        indices (np.ndarray): Array of indices.\n",
    "        num_docs (int): Number of documents.\n",
    "        query_token_ids (np.ndarray): Array of query token IDs.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: Array of relevance scores for the query\n",
    "        \"\"\"\n",
    "        data = self.scores[\"data\"] \n",
    "        indptr = self.scores[\"indptr\"]\n",
    "        indices = self.scores[\"indices\"]\n",
    "        num_docs = self.scores[\"num_docs\"]\n",
    "        scores = np.zeros(num_docs, dtype=FLOAT_TYPE)\n",
    "        for i in range(len(query_token_ids)):\n",
    "            start, end = indptr[query_token_ids[i]], indptr[query_token_ids[i] + 1]\n",
    "            np.add.at(scores, indices[start:end], data[start:end])\n",
    "        return scores\n",
    "\n",
    "    def build_index(self, unique_token_ids, corpus_token_ids, show_progress=True):\n",
    "        \"\"\"\n",
    "        Build the index for the corpus.\n",
    "\n",
    "        Parameters:\n",
    "        unique_token_ids (List[int]): List of unique token IDs.\n",
    "        corpus_token_ids (List[List[int]]): List of tokenized documents.\n",
    "        show_progress (bool): Whether to show a progress bar.\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary containing score matrix data, indices, index pointer, and number of documents.\n",
    "        \"\"\"\n",
    "        avg_doc_len = np.mean([len(doc_ids) for doc_ids in corpus_token_ids])\n",
    "        total_docs = len(corpus_token_ids)\n",
    "        doc_freqs = compute_doc_frequencies(corpus_token_ids, unique_token_ids, show_progress)\n",
    "        idf_array = create_idf_array(doc_freqs, total_docs)\n",
    "        scores, doc_indices, vocab_indices = create_score_matrix(corpus_token_ids, idf_array, avg_doc_len, doc_freqs, self.k1, self.b, show_progress)\n",
    "        score_matrix = sp.csc_matrix((scores, (doc_indices, vocab_indices)), shape=(total_docs, len(unique_token_ids)), dtype=FLOAT_TYPE) # efficiently stores BM25 scores for each term in each document\n",
    "        return {\"data\": score_matrix.data, \"indices\": score_matrix.indices, \"indptr\": score_matrix.indptr, \"num_docs\": total_docs}\n",
    "\n",
    "    def index_corpus(self, corpus: TokenData, show_progress=True):\n",
    "        \"\"\"\n",
    "        Index the corpus.\n",
    "\n",
    "        Parameters:\n",
    "        corpus (TokenData): TokenData object containing tokenized documents and vocabulary.\n",
    "        show_progress (bool): Whether to show a progress bar.\n",
    "        \"\"\"\n",
    "        self.scores = self.build_index(list(corpus.vocab.values()), corpus.ids, show_progress)\n",
    "        self.vocab_dict = corpus.vocab\n",
    "\n",
    "    def dump(self, scores_path, vocab_path):\n",
    "        \"\"\"\n",
    "        Save the BM25 index and vocabulary to disk.\n",
    "\n",
    "        Parameters:\n",
    "        scores_path (str): Path to save the BM25 scores.\n",
    "        vocab_path (str): Path to save the vocabulary dictionary.\n",
    "        \"\"\"\n",
    "        # Save self.scores\n",
    "        np.savez_compressed(scores_path, \n",
    "                            data=self.scores['data'], \n",
    "                            indices=self.scores['indices'], \n",
    "                            indptr=self.scores['indptr'], \n",
    "                            num_docs=self.scores['num_docs'])\n",
    "        \n",
    "        # Save self.vocab_dict\n",
    "        with open(vocab_path, 'w') as vocab_file:\n",
    "            json.dump(self.vocab_dict, vocab_file)\n",
    "\n",
    "    def load(self, scores_path, vocab_path):\n",
    "        \"\"\"\n",
    "        Load the BM25 index and vocabulary from disk.\n",
    "\n",
    "        Parameters:\n",
    "        scores_path (str): Path to load the BM25 scores from.\n",
    "        vocab_path (str): Path to load the vocabulary dictionary from.\n",
    "        \"\"\"\n",
    "        # Load self.scores\n",
    "        loaded = np.load(scores_path)\n",
    "        self.scores = {\n",
    "            \"data\": loaded['data'], \n",
    "            \"indices\": loaded['indices'], \n",
    "            \"indptr\": loaded['indptr'], \n",
    "            \"num_docs\": loaded['num_docs']\n",
    "        }\n",
    "        \n",
    "        # Load self.vocab_dict\n",
    "        with open(vocab_path, 'r') as vocab_file:\n",
    "            self.vocab_dict = json.load(vocab_file)\n",
    "\n",
    "    def get_token_ids(self, query_tokens: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Get token IDs for a query.\n",
    "\n",
    "        Parameters:\n",
    "        query_tokens (List[str]): List of query tokens.\n",
    "\n",
    "        Returns:\n",
    "        List[int]: List of token IDs.\n",
    "        \"\"\"\n",
    "        return [self.vocab_dict[token] for token in query_tokens if token in self.vocab_dict] # words not in vocab are ignored\n",
    "\n",
    "    def retrieve_top_k(self, query_tokens: List[str], lang: str, k: int = 10, doc_index_to_lang = None, filter_by_lang: bool = False):\n",
    "        \"\"\"\n",
    "        Retrieve the top k documents for a query.\n",
    "\n",
    "        Parameters:\n",
    "        query_tokens (List[str]): List of query tokens.\n",
    "        lang (str): Language of the documents to search.\n",
    "        k (int): Number of top documents to retrieve.\n",
    "        doc_index_to_lang (dict): Dictionary mapping document indices to their languages.\n",
    "        filter_by_lang (bool): Whether to filter documents by language.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Arrays of top k scores and their indices.\n",
    "        \"\"\"\n",
    "        query_tokens_ids = self.get_token_ids(query_tokens)\n",
    "        scores = self.compute_relevance_scores(np.asarray(query_tokens_ids, dtype=INT_TYPE))\n",
    "\n",
    "        if filter_by_lang and lang is not None:\n",
    "            # Filter scores based on document language\n",
    "            filtered_scores = [(score, idx) for idx, score in enumerate(scores) if doc_index_to_lang[idx] == lang]\n",
    "            if not filtered_scores:\n",
    "                return np.array([]), np.array([])\n",
    "\n",
    "            # Convert filtered scores to arrays\n",
    "            filtered_scores_array = np.array([score for score, _ in filtered_scores], dtype=FLOAT_TYPE)\n",
    "            filtered_indices_array = np.array([idx for _, idx in filtered_scores], dtype=INT_TYPE)\n",
    "\n",
    "            # Get top k scores and their indices\n",
    "            top_k_scores, top_k_indices = get_top_k(filtered_scores_array, k)\n",
    "\n",
    "            # Map top k indices back to original document indices\n",
    "            top_k_indices = filtered_indices_array[top_k_indices]\n",
    "        else:\n",
    "            # Get top k scores and their indices without filtering by language\n",
    "            top_k_scores, top_k_indices = get_top_k(scores, k)\n",
    "\n",
    "        return top_k_scores, top_k_indices\n",
    "\n",
    "    def search(self, queries: TokenData, langs: List[str], k: int = 10, show_progress: bool = True, n_threads: int = 0, chunksize: int = 50, doc_index_to_lang = None, filter_by_lang: bool = False):\n",
    "        \"\"\"\n",
    "        Retrieve the top-k documents for each query.\n",
    "\n",
    "        Parameters:\n",
    "        queries (TokenData): A TokenData object with tokenized queries.\n",
    "        langs (List[str]): List of languages corresponding to each query.\n",
    "        k (int): Number of documents to retrieve for each query.\n",
    "        show_progress (bool): Whether to show a progress bar.\n",
    "        n_threads (int): Number of jobs to run in parallel. If -1, it will use all available CPUs. If 0, it will run the jobs sequentially.\n",
    "        chunksize (int): Number of batches to process in each job in the multiprocessing pool.\n",
    "        doc_index_to_lang (dict): Dictionary mapping document indices to their languages.\n",
    "        filter_by_lang (bool): Whether to filter documents by language.\n",
    "\n",
    "        Returns:\n",
    "        Tuple of top k document ids retrieved and their scores.\n",
    "        \"\"\"\n",
    "        if n_threads == -1:\n",
    "            n_threads = os.cpu_count()\n",
    "        queries = tokens_to_strings(queries)\n",
    "\n",
    "        queries_with_langs = zip(queries, langs)\n",
    "\n",
    "        topk_fn = partial(self.retrieve_top_k, k=k, doc_index_to_lang=doc_index_to_lang, filter_by_lang=filter_by_lang) # new function with all these args already set\n",
    "        if n_threads == 0:\n",
    "            out = list(tqdm(map(lambda ql: topk_fn(ql[0], ql[1]), queries_with_langs), total=len(queries), desc=\"Retrieving Documents\", disable=not show_progress))\n",
    "        else:\n",
    "            with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "                out = list(tqdm(executor.map(lambda ql: topk_fn(ql[0], ql[1]), queries_with_langs, chunksize=chunksize), total=len(queries), desc=\"Retrieving Documents\", disable=not show_progress))\n",
    "        scores, indices = zip(*out)\n",
    "        return SearchResults(documents=np.array(indices), scores=np.array(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:12.152585Z",
     "iopub.status.busy": "2024-11-01T17:09:12.151971Z",
     "iopub.status.idle": "2024-11-01T17:09:13.044464Z",
     "shell.execute_reply": "2024-11-01T17:09:13.042988Z",
     "shell.execute_reply.started": "2024-11-01T17:09:12.152524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KoreanStemmer:\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "\n",
    "    def stem_words(self, words):\n",
    "        return [self.kkma.morphs(word) for word in words]\n",
    "\n",
    "stemmers = {\n",
    "    \"en\": Stemmer.Stemmer(\"english\"),\n",
    "    \"fr\": Stemmer.Stemmer(\"french\"),\n",
    "    # \"es\": Stemmer.Stemmer(\"spanish\"),\n",
    "    \"de\": Stemmer.Stemmer(\"german\"),\n",
    "    \"it\": Stemmer.Stemmer(\"italian\"),\n",
    "    \"ar\": Stemmer.Stemmer(\"arabic\"),\n",
    "    \"ko\": KoreanStemmer()\n",
    "}\n",
    "\n",
    "class TokenData(NamedTuple):\n",
    "    ids: List[List[int]]\n",
    "    vocab: Dict[str, int]\n",
    "\n",
    "def get_stopwords(language: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get stopwords for a given language.\n",
    "    \"\"\"\n",
    "    if language in [\"english\", \"en\"]: \n",
    "        return STOPWORDS_EN\n",
    "    elif language in [\"english_plus\", \"en_plus\"]:\n",
    "        return STOPWORDS_EN_PLUS\n",
    "    elif language in [\"german\", \"de\"]:\n",
    "        return STOPWORDS_GERMAN\n",
    "    elif language in [\"dutch\", \"nl\"]:\n",
    "        return STOPWORDS_DUTCH\n",
    "    elif language in [\"french\", \"fr\"]:\n",
    "        return STOPWORDS_FRENCH\n",
    "    # elif language in [\"spanish\", \"es\"]:\n",
    "    #     return STOPWORDS_SPANISH\n",
    "    elif language in [\"portuguese\", \"pt\"]:\n",
    "        return STOPWORDS_PORTUGUESE\n",
    "    elif language in [\"italian\", \"it\"]:\n",
    "        return STOPWORDS_ITALIAN\n",
    "    elif language in [\"russian\", \"ru\"]:\n",
    "        return STOPWORDS_RUSSIAN\n",
    "    elif language in [\"swedish\", \"sv\"]:\n",
    "        return STOPWORDS_SWEDISH\n",
    "    elif language in [\"norwegian\", \"no\"]:\n",
    "        return STOPWORDS_NORWEGIAN\n",
    "    elif language in [\"chinese\", \"zh\"]:\n",
    "        return STOPWORDS_CHINESE\n",
    "    elif language in [\"arabic\", \"ar\"]:\n",
    "        return STOPWORDS_ARABIC\n",
    "    elif language in [\"korean\", \"ko\"]:\n",
    "        return STOPWORDS_KOREAN\n",
    "    else:\n",
    "        # print(f\"{language} stopwords not supported, defaulting to English stopwords\")\n",
    "        return STOPWORDS_EN\n",
    "\n",
    "def tokenize(\n",
    "    texts: List[Tuple[str, str]],  # list of tuples (text, language)\n",
    "    lower: bool = True,\n",
    "    return_ids: bool = True,\n",
    "    show_progress: bool = True,\n",
    "    leave: bool = False,\n",
    ") -> Union[List[List[str]], TokenData]:\n",
    "    \"\"\"\n",
    "    Tokenize a list of texts with optional stemming and stopwords removal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : List[Tuple[str, str]]\n",
    "        A list of tuples where each tuple contains a text and its language.\n",
    "\n",
    "    lower : bool, optional\n",
    "        Convert text to lowercase before tokenization.\n",
    "\n",
    "    return_ids : bool, optional\n",
    "        Return token IDs and vocabulary if True, else return tokenized strings.\n",
    "\n",
    "    show_progress : bool, optional\n",
    "        Show progress bar if True.\n",
    "\n",
    "    leave : bool, optional\n",
    "        Leave progress bar after completion if True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Union[List[List[str]], TokenData]\n",
    "        Tokenized texts as strings or token IDs with vocabulary.\n",
    "    \"\"\"\n",
    "    token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "    split_fn = re.compile(token_pattern).findall\n",
    "\n",
    "    corpus_ids = []\n",
    "    token_to_index = {}\n",
    "\n",
    "    for text, language in tqdm(texts, desc=\"Tokenizing texts\", leave=leave, disable=not show_progress):\n",
    "        stopwords_set = set(get_stopwords(language))\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        tokens = split_fn(text)\n",
    "        doc_ids = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in stopwords_set:\n",
    "                continue\n",
    "\n",
    "            if token not in token_to_index:\n",
    "                token_to_index[token] = len(token_to_index)\n",
    "\n",
    "            token_id = token_to_index[token]\n",
    "            doc_ids.append(token_id)\n",
    "\n",
    "        corpus_ids.append(doc_ids)\n",
    "\n",
    "    unique_tokens = list(token_to_index.keys())\n",
    "\n",
    "    stemmer = stemmers.get(language, Stemmer.Stemmer(\"english\"))\n",
    "    if hasattr(stemmer, \"stemWords\"):\n",
    "        stemmer_fn = stemmer.stemWords\n",
    "    elif callable(stemmer):\n",
    "        stemmer_fn = stemmer\n",
    "    else:\n",
    "        raise ValueError(\"Stemmer must have a `stemWords` method or be callable\")\n",
    "\n",
    "    stemmed_tokens = stemmer_fn(unique_tokens)\n",
    "    vocab = set(stemmed_tokens)\n",
    "    vocab_dict = {token: i for i, token in enumerate(vocab)}\n",
    "    stem_id_to_stem = {v: k for k, v in vocab_dict.items()}\n",
    "    doc_id_to_stem_id = {token_to_index[token]: vocab_dict[stem] for token, stem in zip(unique_tokens, stemmed_tokens)}\n",
    "\n",
    "    for i, doc_ids in enumerate(tqdm(corpus_ids, desc=\"Stemming tokens\", leave=leave, disable=not show_progress)):\n",
    "        corpus_ids[i] = [doc_id_to_stem_id[doc_id] for doc_id in doc_ids]\n",
    "\n",
    "    if return_ids:\n",
    "        return TokenData(ids=corpus_ids, vocab=vocab_dict)\n",
    "    else:\n",
    "        reverse_dict = stem_id_to_stem if stemmers is not None else unique_tokens\n",
    "        for i, token_ids in enumerate(tqdm(corpus_ids, desc=\"Reconstructing token strings\", leave=leave, disable=not show_progress)):\n",
    "            corpus_ids[i] = [reverse_dict[token_id] for token_id in token_ids]\n",
    "        return corpus_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two objects are loaded into the model :\n",
    "- Scores: a dictionary that contains the BM25 score matrix components. This sparse matrix is a way to efficiently store BM25 scores for each term in each document, allowing for fast retrieval and ranking of documents.\n",
    "- Vocab: a dictionary mapping each token to its ID. It is used to convert query tokens into their corresponding token IDs, which are then used to look up BM25 scores in the score matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:13.048075Z",
     "iopub.status.busy": "2024-11-01T17:09:13.047681Z",
     "iopub.status.idle": "2024-11-01T17:09:38.304381Z",
     "shell.execute_reply": "2024-11-01T17:09:38.302725Z",
     "shell.execute_reply.started": "2024-11-01T17:09:13.048037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk. It took 25.250093936920166 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load model from disk\n",
    "start_time = time.time()\n",
    "retriever = BM25() \n",
    "retriever.load('/kaggle/input/saved-objects4/bm25_scores.npz', '/kaggle/input/saved-objects4/bm25_vocab.json')\n",
    "end_time = time.time()\n",
    "print(f\"Loaded model from disk. It took {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:38.306218Z",
     "iopub.status.busy": "2024-11-01T17:09:38.305829Z",
     "iopub.status.idle": "2024-11-01T17:09:38.812224Z",
     "shell.execute_reply": "2024-11-01T17:09:38.810942Z",
     "shell.execute_reply.started": "2024-11-01T17:09:38.306178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dict to match document index and corresponding docid\n",
    "with open('/kaggle/input/saved-objects/doc_index_to_docid.json', 'r') as f:\n",
    "    doc_index_to_docid = json.load(f)\n",
    "doc_index_to_docid = {int(key): value for key, value in doc_index_to_docid.items()} # reconvert keys to int\n",
    "\n",
    "# Load dict to match document index and corresponding lang\n",
    "with open('/kaggle/input/saved-objects/doc_index_to_lang.json', 'r') as f:\n",
    "    doc_index_to_lang = json.load(f)\n",
    "doc_index_to_lang = {int(key): value for key, value in doc_index_to_lang.items()} # reconvert keys to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:38.813964Z",
     "iopub.status.busy": "2024-11-01T17:09:38.813579Z",
     "iopub.status.idle": "2024-11-01T17:09:38.942820Z",
     "shell.execute_reply": "2024-11-01T17:09:38.941775Z",
     "shell.execute_reply.started": "2024-11-01T17:09:38.813924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test set\n",
    "test_set_path = '/kaggle/input/dis-project-1-document-retrieval/test.csv'\n",
    "test_set = pd.read_csv(test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:38.945495Z",
     "iopub.status.busy": "2024-11-01T17:09:38.944545Z",
     "iopub.status.idle": "2024-11-01T17:09:44.857823Z",
     "shell.execute_reply": "2024-11-01T17:09:44.856437Z",
     "shell.execute_reply.started": "2024-11-01T17:09:38.945436Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d5565a2be34f349d3d3d9f92d1bbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing texts:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f84c907825241b3ac955263208d447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stemming tokens:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized queries. It took 0.14459896087646484 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b310e7545474f3c84777269bac9b667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving Documents:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched and retrieved similar documents. It took 5.620001792907715 seconds\n"
     ]
    }
   ],
   "source": [
    "# Retrieve docs for each query in test set\n",
    "queries_and_lang = []\n",
    "langs = []\n",
    "for index, row in test_set.iterrows():\n",
    "    queries_and_lang.append((row['query'], row['lang'])) # to preprocess according to lang\n",
    "    langs.append(row['lang']) # to associate query with its lang during retrieval\n",
    "\n",
    "start_time = time.time()\n",
    "queries_tokens = tokenize(queries_and_lang)\n",
    "end_time = time.time()\n",
    "print(f\"Tokenized queries. It took {end_time - start_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "retrieved_docs_indices, scores = retriever.search(queries_tokens, langs, k=10, n_threads=-1, doc_index_to_lang=doc_index_to_lang, filter_by_lang=False)\n",
    "end_time = time.time()\n",
    "print(f\"Searched and retrieved similar documents. It took {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T17:09:44.859924Z",
     "iopub.status.busy": "2024-11-01T17:09:44.859420Z",
     "iopub.status.idle": "2024-11-01T17:09:45.084894Z",
     "shell.execute_reply": "2024-11-01T17:09:45.083511Z",
     "shell.execute_reply.started": "2024-11-01T17:09:44.859871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get documents retrieved for each query in test set\n",
    "predicted_docs = []\n",
    "for index, row in test_set.iterrows():\n",
    "    query_id = row['id']\n",
    "    retrieve_docs_ids = [doc_index_to_docid[doc_index] for doc_index in retrieved_docs_indices[index]]\n",
    "    predicted_docs.append((query_id, retrieve_docs_ids))\n",
    "\n",
    "# Create Dataframe with results\n",
    "results_df = pd.DataFrame(predicted_docs, columns=['id', 'docids'])\n",
    "\n",
    "# Save to csv\n",
    "results_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9635715,
     "sourceId": 85316,
     "sourceType": "competition"
    },
    {
     "datasetId": 5986166,
     "sourceId": 9772736,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5986602,
     "sourceId": 9773309,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5992600,
     "sourceId": 9781569,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5992684,
     "sourceId": 9781678,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5993124,
     "sourceId": 9782277,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
