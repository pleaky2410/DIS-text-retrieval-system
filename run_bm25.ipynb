{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the corpus has not been tokenized yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus json\n",
    "import json\n",
    "import gc\n",
    "\n",
    "print('Load corpus.json')\n",
    "with open('data/corpus.json/corpus.json', 'r') as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text and lang from docs\n",
    "\n",
    "texts_and_lang = []\n",
    "for doc in documents:\n",
    "    texts_and_lang.append((doc[\"text\"], doc[\"lang\"]))\n",
    "\n",
    "del documents\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus by language and combine the results\n",
    "from preprocessing.tokenization2 import tokenize\n",
    "\n",
    "corpus_tokens = tokenize(texts_and_lang)\n",
    "\n",
    "del texts_and_lang\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus_tokens on disk\n",
    "import pickle\n",
    "\n",
    "# Save aggregated_docs_vectors to disk\n",
    "with open(f'saved_objects/corpus_tokens2.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_tokens, f)\n",
    "print(f\"Saved saved_objects/corpus_tokens.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the corpus has already been tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the corpus_tokens from disk\n",
    "import pickle\n",
    "\n",
    "with open('saved_objects/corpus_tokens.pkl', 'rb') as f:\n",
    "    corpus_tokens = pickle.load(f)\n",
    "print(\"Loaded corpus_tokens from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the model has not indexed the corpus yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bm25 import BM25\n",
    "import gc\n",
    "# Create the BM25 model and index the corpus\n",
    "k1 = 1.5\n",
    "b = 0.75\n",
    "retriever = BM25(k1=k1, b=b)\n",
    "retriever.index_corpus(corpus_tokens)\n",
    "del corpus_tokens\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk\n",
    "retriever.dump('saved_objects/bm25_scores.npz', 'saved_objects/bm25_vocab.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the model has already indexed the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from disk\n",
    "retriever = BM25() \n",
    "retriever.load('saved_objects/bm25_scores.npz', 'saved_objects/bm25_vocab.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate recall on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dict to match document index and corresponding docid\n",
    "import json\n",
    "with open('saved_objects/doc_index_to_docid.json', 'r') as f:\n",
    "    doc_index_to_docid = json.load(f)\n",
    "doc_index_to_docid = {int(key): value for key, value in doc_index_to_docid.items()} # reconvert keys to int\n",
    "\n",
    "# Load dict to match document index and corresponding lang\n",
    "import json\n",
    "with open('saved_objects/doc_index_to_lang.json', 'r') as f:\n",
    "    doc_index_to_lang = json.load(f)\n",
    "doc_index_to_lang = {int(key): value for key, value in doc_index_to_lang.items()} # reconvert keys to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate recall@10\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    retrieved_set = set(retrieved_docs[:k])\n",
    "    relevant_set = set(relevant_docs)\n",
    "    intersection = retrieved_set.intersection(relevant_set)\n",
    "    recall = len(intersection) / len(relevant_set)\n",
    "    return recall\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dev set\n",
    "dev_set_path = 'data/dev.csv'\n",
    "dev_set = pd.read_csv(dev_set_path)\n",
    "\n",
    "\n",
    "# Positive/Negative docs to list\n",
    "def docs_to_list(docs):\n",
    "    if isinstance(docs, str):\n",
    "        if docs.startswith('[') and docs.endswith(']'):\n",
    "            return eval(docs)\n",
    "        else:\n",
    "            return [docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve docs for each query in dev set\n",
    "from preprocessing.tokenization import tokenize\n",
    "\n",
    "queries_and_lang = []\n",
    "langs = []\n",
    "for index, row in dev_set.iterrows():\n",
    "    queries_and_lang.append((row['query'], row['lang'])) # to preprocess according to lang\n",
    "    langs.append(row['lang']) # to associate query with its lang during retrieval\n",
    "\n",
    "queries_tokens = tokenize(queries_and_lang)\n",
    "\n",
    "retrieved_docs_indices, scores = retriever.search(queries_tokens, langs, k=10, n_threads=-1, doc_index_to_lang=doc_index_to_lang, filter_by_lang=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate recall@10 for each query in dev set\n",
    "recalls = []\n",
    "lang_recalls = {}\n",
    "for index, row in dev_set.iterrows():\n",
    "    lang = row['lang']\n",
    "    retrieve_docs_ids = [doc_index_to_docid[doc_index] for doc_index in retrieved_docs_indices[index]]\n",
    "    positive_docs = docs_to_list(row['positive_docs']) # convert str to python list\n",
    "    recall = calculate_recall_at_k(retrieve_docs_ids, positive_docs, k=10)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    # Add recall to specific langage\n",
    "    if lang not in lang_recalls:\n",
    "        lang_recalls[lang] = []\n",
    "    lang_recalls[lang].append(recall)\n",
    "\n",
    "# Calculate average recall\n",
    "mean_recall_at_10 = np.mean(recalls)\n",
    "print(f\"Mean Recall@10: {mean_recall_at_10:.4f}\")\n",
    "\n",
    "# Calculate average recall for each language\n",
    "for lang, lang_recall_list in lang_recalls.items():\n",
    "    mean_lang_recall = np.mean(lang_recall_list)\n",
    "    print(f\"Mean Recall@10 for {lang}: {mean_lang_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bm25_stopwords_each_lang_stopwords_stemmer_each_lang_retrieve_docs_by_lang_no_spanish\"\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"word_embedding\")\n",
    "\n",
    "# Start an MLflow run# MLflow logging\n",
    "with mlflow.start_run(run_name=model_name):\n",
    "    mlflow.log_param(\"framework\", \"BM25S\")\n",
    "    mlflow.log_param(\"model\", \"BM25S\")\n",
    "    mlflow.log_param(\"input\", \"full_corpus\")\n",
    "    \n",
    "    mlflow.log_metric(\"k1\", k1)\n",
    "    mlflow.log_metric(\"b\", b)\n",
    "\n",
    "    mlflow.log_metric(\"recall_at10_dev\", mean_recall_at_10)\n",
    "    for lang, lang_recall_list in lang_recalls.items():\n",
    "        mean_lang_recall = np.mean(lang_recall_list)\n",
    "        mlflow.log_metric(f\"recall_at10_dev_{lang}\", mean_lang_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "test_set_path = 'data/test.csv'\n",
    "test_set = pd.read_csv(test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve docs for each query in test set\n",
    "from preprocessing.tokenization import tokenize\n",
    "\n",
    "queries_and_lang = []\n",
    "langs = []\n",
    "for index, row in test_set.iterrows():\n",
    "    queries_and_lang.append((row['query'], row['lang'])) # to preprocess according to lang\n",
    "    langs.append(row['lang']) # to associate query with its lang during retrieval\n",
    "\n",
    "queries_tokens = tokenize(queries_and_lang)\n",
    "\n",
    "retrieved_docs_indices, scores = retriever.search(queries_tokens, langs, k=10, n_threads=-1, doc_index_to_lang=doc_index_to_lang, filter_by_lang=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents retrieved for each query in test set\n",
    "predicted_docs = []\n",
    "for index, row in test_set.iterrows():\n",
    "    query_id = row['id']\n",
    "    retrieve_docs_ids = [doc_index_to_docid[doc_index] for doc_index in retrieved_docs_indices[index]]\n",
    "    predicted_docs.append((query_id, retrieve_docs_ids))\n",
    "\n",
    "# Create Dataframe with results\n",
    "results_df = pd.DataFrame(predicted_docs, columns=['id', 'docids'])\n",
    "\n",
    "# Save to csv\n",
    "results_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
