{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the corpus has not been tokenized yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus json\n",
    "import json\n",
    "import gc\n",
    "\n",
    "print('Load corpus.json')\n",
    "with open('data/corpus.json/corpus.json', 'r') as f:\n",
    "    documents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from docs\n",
    "print('Extract text from docs')\n",
    "texts = [doc['text'] for doc in documents]\n",
    "\n",
    "del documents\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import Stemmer  # optional: for stemming\n",
    "\n",
    "# optional: create a stemmer\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "# Tokenize the corpus and only keep the ids (faster and saves memory)\n",
    "corpus_tokens = bm25s.tokenize(texts, stopwords=\"en\", stemmer=stemmer)\n",
    "\n",
    "del texts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corpus_tokens on disk\n",
    "import pickle\n",
    "\n",
    "# Save aggregated_docs_vectors to disk\n",
    "with open(f'saved_objects/corpus_tokens.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_tokens, f)\n",
    "print(f\"Saved saved_objects/corpus_tokens.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the corpus has already been tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the corpus_tokens from disk\n",
    "import pickle\n",
    "\n",
    "with open('saved_objects/corpus_tokens.pkl', 'rb') as f:\n",
    "    corpus_tokens = pickle.load(f)\n",
    "print(\"Loaded corpus_tokens from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bm25 import BM25\n",
    "# Create the BM25 model and index the corpus\n",
    "retriever = BM25()\n",
    "retriever.index_corpus(corpus_tokens)\n",
    "del corpus_tokens\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate recall on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "\n",
    "# optional: create a stemmer\n",
    "stemmer = Stemmer.Stemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dict to match document index and corresponding docid\n",
    "import json\n",
    "with open('saved_objects/doc_index_to_docid.json', 'r') as f:\n",
    "    doc_index_to_docid = json.load(f)\n",
    "doc_index_to_docid = {int(key): value for key, value in doc_index_to_docid.items()} # reconvert keys to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate recall@10\n",
    "def calculate_recall_at_k(retrieved_docs, relevant_docs, k=10):\n",
    "    retrieved_set = set(retrieved_docs[:k])\n",
    "    relevant_set = set(relevant_docs)\n",
    "    intersection = retrieved_set.intersection(relevant_set)\n",
    "    recall = len(intersection) / len(relevant_set)\n",
    "    return recall\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dev set\n",
    "dev_set_path = 'data/dev.csv'\n",
    "dev_set = pd.read_csv(dev_set_path)\n",
    "\n",
    "\n",
    "# Positive/Negative docs to list\n",
    "def docs_to_list(docs):\n",
    "    if isinstance(docs, str):\n",
    "        if docs.startswith('[') and docs.endswith(']'):\n",
    "            return eval(docs)\n",
    "        else:\n",
    "            return [docs]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve docs for each query in dev set\n",
    "queries = []\n",
    "for index, row in dev_set.iterrows():\n",
    "    queries.append(row['query'])\n",
    "\n",
    "queries_tokens = bm25s.tokenize(queries, stemmer=stemmer)\n",
    "retrieved_docs_indices, scores = retriever.search(queries_tokens, k=10, n_threads=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate recall@10 for each query in dev set\n",
    "recalls = []\n",
    "lang_recalls = {}\n",
    "for index, row in dev_set.iterrows():\n",
    "    lang = row['lang']\n",
    "    retrieve_docs_ids = [doc_index_to_docid[doc_index] for doc_index in retrieved_docs_indices[index]]\n",
    "    positive_docs = docs_to_list(row['positive_docs']) # convert str to python list\n",
    "    recall = calculate_recall_at_k(retrieve_docs_ids, positive_docs, k=10)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    # Add recall to specific langage\n",
    "    if lang not in lang_recalls:\n",
    "        lang_recalls[lang] = []\n",
    "    lang_recalls[lang].append(recall)\n",
    "\n",
    "# Calculate average recall\n",
    "mean_recall_at_10 = np.mean(recalls)\n",
    "print(f\"Mean Recall@10: {mean_recall_at_10:.4f}\")\n",
    "\n",
    "# Calculate average recall for each language\n",
    "for lang, lang_recall_list in lang_recalls.items():\n",
    "    mean_lang_recall = np.mean(lang_recall_list)\n",
    "    print(f\"Mean Recall@10 for {lang}: {mean_lang_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "test_set_path = 'data/test.csv'\n",
    "test_set = pd.read_csv(test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve docs for each query in test set\n",
    "queries = []\n",
    "for index, row in test_set.iterrows():\n",
    "    queries.append(row['query'])\n",
    "\n",
    "queries_tokens = bm25s.tokenize(queries, stemmer=stemmer)\n",
    "retrieved_docs_indices, scores = retriever.search(queries_tokens, k=10, n_threads=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get documents retrieved for each query in test set\n",
    "predicted_docs = []\n",
    "for index, row in test_set.iterrows():\n",
    "    query_id = row['id']\n",
    "    retrieve_docs_ids = [doc_index_to_docid[doc_index] for doc_index in retrieved_docs_indices[index]]\n",
    "    predicted_docs.append((query_id, retrieve_docs_ids))\n",
    "\n",
    "# Create Dataframe with results\n",
    "results_df = pd.DataFrame(predicted_docs, columns=['id', 'docids'])\n",
    "\n",
    "# Save to csv\n",
    "results_df.to_csv('predicted_docs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
