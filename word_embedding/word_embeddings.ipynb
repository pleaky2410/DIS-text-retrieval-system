{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import fasttext\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Function to print memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory Usage: {mem_info.rss / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "\n",
    "# Load corpus json\n",
    "print_memory_usage()\n",
    "print('Load corpus.json')\n",
    "with open('actual_data/corpus.json/corpus.json', 'r') as f:\n",
    "    documents = json.load(f)\n",
    "print_memory_usage()\n",
    "\n",
    "# Extract text from docs\n",
    "print('Extract text from docs')\n",
    "texts = [doc['text'] for doc in documents]\n",
    "print_memory_usage()\n",
    "\n",
    "\n",
    "del documents\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "# Save docs in temp file\n",
    "print('Save docs in temp file')\n",
    "with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "    temp_file_name = temp_file.name\n",
    "    for text in texts:\n",
    "        temp_file.write((text + '\\n').encode('utf-8'))\n",
    "print_memory_usage()\n",
    "\n",
    "print('Delete texts variable')\n",
    "del texts\n",
    "gc.collect()\n",
    "print_memory_usage()\n",
    "\n",
    "print('Start fasttext model training')\n",
    "model = fasttext.train_unsupervised(temp_file_name, model = 'cbow', thread=cpu_count())\n",
    "print_memory_usage()\n",
    "\n",
    "# Close and remove the temporary file\n",
    "temp_file.close()\n",
    "os.remove(temp_file_name)\n",
    "\n",
    "# Save the trained model\n",
    "print('Save the trained model')\n",
    "model.save_model(\"model_word_embeddings_fasttext.bin\")\n",
    "print_memory_usage()\n",
    "\n",
    "print('Delete model variable')\n",
    "del model\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "model = fasttext.load_model(\"model_filename.bin\")\n",
    "\n",
    "vocabulary = model.words\n",
    "word_embeddings = np.array([model[word] for word in vocabulary])\n",
    "\n",
    "# Clean memory\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 3. Visualize Embeddings\n",
    "\n",
    "In the third phase of this exercise, we will visualize the generated embeddings using t-SNE (T-Distributed Stochastic Neighbouring Entities).\n",
    "\n",
    "t-SNE is a dimensionality reduction algorithm which is well suited for such visualization tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000, init = 'pca') \n",
    "vis_data = tsne.fit_transform(word_embeddings)\n",
    "\n",
    "# Clean memory\n",
    "del tsne\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_data_x = vis_data[:,0]\n",
    "vis_data_y = vis_data[:,1]\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.figure(figsize=(40, 40)) \n",
    "plt.scatter(vis_data_x, vis_data_y)\n",
    "\n",
    "for label, x, y in zip(vocabulary, vis_data_x, vis_data_y):\n",
    "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "plt.show()\n",
    "\n",
    "# Clean memory\n",
    "del vis_data\n",
    "del vis_data_x\n",
    "del vis_data_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1. Observe the plot of word embeddings. Do you observe any patterns?\n",
    "\n",
    "2. Write a python function to find the most similar terms for a given term. The similarity between two terms is defined as the cosine similarity between their corresponding word embeddings. Find the top 5 terms that are most similar to 'la', 'EPFL', '#robot', 'this'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(input_term, word_embeddings, vocabulary, num_terms=5):\n",
    "    # Create dict to associate embedding to each term in vocabulary\n",
    "    term_embeddings_dict = {} \n",
    "    for i,term in enumerate(vocabulary):\n",
    "        term_embeddings_dict[term] = word_embeddings[i]\n",
    "    \n",
    "    # Find input_term in embeddings dict\n",
    "    if input_term not in term_embeddings_dict:\n",
    "        return \"Term not in the vocabulary\"\n",
    "    input_term_embedding = term_embeddings_dict[input_term]\n",
    "\n",
    "    # Calculate similarity with each term in vocabulary\n",
    "    term_similarities = []\n",
    "    for term, embedding in term_embeddings_dict.items():\n",
    "        term_similarities.append([term, cosine_similarity(input_term_embedding.reshape((1,-1)), embedding.reshape((1,-1)))]) # reshape embedding into 2D array with 1 line as expected by cosine_similarity function\n",
    "        \n",
    "    sorted_terms = sorted(term_similarities, key = lambda x: -1 * x[1])[0:num_terms] # sort by decreasing similarity score, select num_terms first elements\n",
    "\n",
    "    return sorted_terms\n",
    "    \n",
    "\n",
    "find_most_similar('Canadian', word_embeddings, vocabulary, num_terms=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 2: Basic Search Engine Using Word Embeddings\n",
    "\n",
    "In this exercise, we would put our word embeddings to test by using them for information retrieval. \n",
    "The idea is that, the documents that have the most similar embedding vectors to the one belongs to query should rank higher.\n",
    "The documents may not necessarily include the keywords in the query.\n",
    "\n",
    "\n",
    "### Goal:\n",
    "1. Implement a search engine that uses word embeddings to retrieve relevant documents (Data file: `epfldocs.txt`)\n",
    "2. Compare the results with vector space retrieval model\n",
    "\n",
    "\n",
    "### What you are learning in this exercise:\n",
    "- Learning to use word embeddings for a search engine \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of libraries and documents\n",
    "import json\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"actual_data/corpus.json/corpus.json\", ) as f:\n",
    "    documents = json.load(f)\n",
    "        \n",
    "original_documents = [doc['text'].strip() for doc in documents]\n",
    "docids = [doc['docid'] for doc in documents]\n",
    "\n",
    "# To match document index and corresponding docid\n",
    "doc_index_to_docid = {index: doc_id for index, doc_id in enumerate(docids)}\n",
    "\n",
    "# Clean memory\n",
    "del documents\n",
    "del docids\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aggregate words of each document\n",
    "Since both the documents and the query is of variable size, we should aggregate the vectors of the words in the query by some strategy. This could be taking the minimum vector, maximum vector or the mean. Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of vectors for easier search\n",
    "vector_dict = dict(zip(vocabulary, word_embeddings))\n",
    "\n",
    "def aggregate_vector_list(vlist, aggfunc):\n",
    "    if aggfunc == 'max':\n",
    "        return np.array(vlist).max(axis=0)\n",
    "    elif aggfunc == 'min':\n",
    "        return np.array(vlist).min(axis=0)\n",
    "    elif aggfunc == 'mean':\n",
    "        return np.array(vlist).mean(axis=0)\n",
    "    else:\n",
    "        return np.zeros(np.array(vlist).shape[1])\n",
    "\n",
    "possible_aggfuncs = [\"max\", \"min\", \"mean\"]\n",
    "\n",
    "aggregated_doc_vectors = {} # for each doc, the 3 possible aggregated vectors (min, max, mean)\n",
    "\n",
    "# Aggregate vectors of documents beforehand\n",
    "#Â TODO\n",
    "for aggfunc in possible_aggfuncs:\n",
    "    aggregated_doc_vectors[aggfunc] = np.zeros((len(original_documents), word_embeddings.shape[1]))\n",
    "    for index, doc in enumerate(original_documents):\n",
    "        vlist = [vector_dict[token] for token in fasttext.tokenize(doc) if token in vector_dict]\n",
    "        if(len(vlist) < 1):\n",
    "            continue \n",
    "        else:\n",
    "            aggregated_doc_vectors[aggfunc][index] = aggregate_vector_list(vlist, aggfunc)\n",
    "\n",
    "del word_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "aggregated_doc_vectors_file = 'aggregated_doc_vectors.pkl'\n",
    "\n",
    "# # Load aggregated_doc_vectors from disk\n",
    "# with open(aggregated_doc_vectors_file, 'rb') as f:\n",
    "#     aggregated_doc_vectors = pickle.load(f)\n",
    "# print(\"Loaded aggregated_doc_vectors from disk.\")\n",
    "\n",
    "# Save aggregated_doc_vectors to disk\n",
    "with open(aggregated_doc_vectors_file, 'wb') as f:\n",
    "    pickle.dump(aggregated_doc_vectors, f)\n",
    "print(\"Saved aggregated_doc_vectors to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Aggregate the query\n",
    "Aggregate the query and find the most similar documents using cosine distance between the query's vector and document's aggregated vector.\n",
    "\n",
    "Are they seem to relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = \"EPFL\"\n",
    "\n",
    "def aggregate_query(query, aggfunc):\n",
    "    # Raise an error message for the case when there is no words in the query that is included in the vocabulary\n",
    "    # This should return a vector of shape (1, word_embeddings.shape[1])\n",
    "    tokens = fasttext.tokenize(query)\n",
    "    if(len(tokens) == 1): # only one term, find its associated vector\n",
    "        if(tokens[0] in vocabulary):\n",
    "            return vector_dict[tokens[0]]\n",
    "    elif(len(tokens) > 1): # more than one term, get list of associated vectors for each term\n",
    "        vlist = []\n",
    "        for token in tokens:\n",
    "            if (token in vocabulary):\n",
    "                vlist.append(vector_dict[token])\n",
    "        \n",
    "        return aggregate_vector_list(vlist, aggfunc) # return aggregated vector according to aggfunc method of aggregating\n",
    "    else:\n",
    "        print(\"%s is not in the vocabulary.\" % (query))\n",
    "    \n",
    "def get_most_similar_documents(query_vector, aggfunc, k = 5):\n",
    "    # Calculate the similarity with each document vector. \n",
    "    #Â Hint: Cosine similarity function takes a matrix as input so you do not need to loop through each document vector.\n",
    "    sim = cosine_similarity(query_vector.reshape((1,-1)), aggregated_doc_vectors[aggfunc])\n",
    "    \n",
    "    # Rank the document vectors according to their cosine similarity with the query vector and return topk indexes\n",
    "    indexes = np.argsort(sim, axis=-1, kind='quicksort', order=None) # This is sorted in ascending order, along last axis\n",
    "    indexes = indexes[0]\n",
    "    indexes = indexes[::-1] # Convert to descending\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def search_vec_embeddings(query, topk = 10, aggfunc = 'mean'):\n",
    "    query_vector = aggregate_query(query, aggfunc)\n",
    "    indexes = get_most_similar_documents(query_vector, aggfunc)\n",
    "    # Print the top k documents\n",
    "    indexes = indexes[0:topk]\n",
    "\n",
    "    print(f\"Document indexes retrieved : {indexes}\")\n",
    "    docids_retrieved = [doc_index_to_docid[index] for index in indexes]\n",
    "    print(f\"Docids retrieved : {docids_retrieved}\")\n",
    "    for index in indexes:\n",
    "        print(\"---------\")\n",
    "        print(original_documents[index])\n",
    "        print(\"---------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings(\"What other companies did the FRC investigate KPMG's role in?\", aggfunc = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare the results with the vector space retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Â VECTOR SPACE RETRIEVAL (From Exercise 1)\n",
    "# Retrieval oracle \n",
    "from operator import itemgetter\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, topk = 10, features = features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = cosine_similarity(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold or i >= topk:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    \n",
    "    for index in doc_ids:\n",
    "        print(original_documents[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_vec_embeddings('EPFL', aggfunc = 'mean')\n",
    "print(\"---------------------------------\")\n",
    "search_vec_sklearn(\"EPFL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_by_docid(docid, file_path='actual_data/corpus.json/corpus.json'):\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        documents = json.load(f)\n",
    "    \n",
    "    for doc in documents:\n",
    "        if doc['docid'] == docid:\n",
    "            return doc\n",
    "    \n",
    "    return None\n",
    "\n",
    "docid_to_find = 'doc-en-792955'\n",
    "document = read_document_by_docid(docid_to_find)\n",
    "\n",
    "if document:\n",
    "    print(f\"Document with docid {docid_to_find}:\")\n",
    "    print(document)\n",
    "else:\n",
    "    print(f\"No document found with docid {docid_to_find}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "You will realize that not all the words in your queries are in the vocabulary, so your queries fail to retrieve any documents. Think of possible solutions to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
